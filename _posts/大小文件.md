title: 利用分块和分片优化存储方案
---

利用Hadoop原生配置设计hdfs存储策略

<!-- more -->

### 对于大文件的优化

HDFS本身设计就是用来存储海量数据，本模块探究了如何配置hadoop的默认信息来看对于大文件的存储和操作是否有优化空间。

分块大小设计

分片大小设计

map数量设计



Snappy，一个来自于Google的压缩编码器，在压缩大小和读写时间上取得了最佳平衡。然而，如果要考虑到处理超大文件时对可分块特性的支持，LZOP将是最佳的编码器。

在评估编码器之前，先要明确评估标准。评估标准是基于功能性需求和性能需求制定的。评估标准可能包括如下内容：

- 空间/时间取向 —— 一般来说，压缩文件越小（并且消耗时间越长），压缩比率越好。
- 可分块特性 —— 压缩文件是否能够被分块，以便在多个map任务中处理？如果压缩文件不能分块，那么只有一个map任务可以处理它。如果文件占据了很多HDFS块，将损失数据的本地性。也就是说，map任务很可能需要从远程数据节点读取苦块，增大网络IO的负担。
- 是否是本地库内置的压缩支持 —— 采用的压缩编码器是否是本地的库。如果采用非本地的JAVA编码器，性能很可能下降。



在HDFS中压缩数据有很多好处，减小文件大小，加快MapReduce作业运行。在Hadoop中有多个压缩编码器可以选择。它们的特性和性能各部相同。

### 对于小文件的优化

小文件指的是文件大小远远小于一个HDFS块（128MB）的大小的文件，比如1k~2M

会产生大量小文件的情况：

- 这些小文件都是一个大的逻辑文件的pieces。由于HDFS仅仅在不久前才刚刚支持对文件的append，因此以前用来向unbounde files(例如log文件)添加内容的方式都是通过将这些数据用许多chunks的方式写入HDFS中
- 文件本身就是很小。例如许许多多的小图片文件。每一个图片都是一个独立的文件。并且没有一种很有效的方法来将这些文件合并为一个大的文件

在HDFS中存储小文件现在有如下两个问题：

- 消耗NameNode大量的内存：小文件会占据多个block，namenode将block的信息保存在内存中，大量的block的信息将会超出namenode硬件所能满足的极限
- 延长MapReduce作业的总运行时间：对于小文件的读取通常会造成大量从datanode到datanode的seeks和hopping来retrieve文件，这是一种十分低效的方式。在进行运算的时候，无法最大限度地利用Locality特性带来的优势，导致大量的数据在集群中传输，开销很大。



Hadoop减轻这样的压力的方法：

- JVM中允许task reuse，以支持在一个JVM中运行多个map task
  - 通过设置mapred.job.reuse.jvm.num.tasks属性，默认为1，－1为无限制
- 使用MultiFileInputSplit，它可以使得一个map中能够处理多个split。

由于Hadoop主要针对大文件的存储与处理，本模块探究了如何配置hadoop的默认信息来优化Hadoop中的小文件存储。



将HDFS中的小文件打包到一个大的文件容器中。这个技术中将本地磁盘中所有的目标文件存储到HDFS中的一个单独的Avro文件。然后在MapReduce中处理Avro文件和其中的小文件。



使用CombineFileInputFormat来优化Hadoop小文件



可以用Hadoop的SequenceFile来处理小文件。

Hadoop还提供了CombineFileInputFormat。它能够让一个单独的map任务处理来自多个文件的多个输入块，以极大地减少需要运行的map任务个数。

以在Hadoop中配置，使map任务的JVM可以处理多个任务，来减少JVM循环的开支。配置项mapred.job.reuse.jvm.num.tasks默认为1.这说明一个JVM只能处理一个任务。当它被配置为更大的数字的时候，一个JVM可以处理多个任务。-1则代表着处理的任务数量无上限。



#### SequenceFile

