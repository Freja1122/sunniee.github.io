title: 两个项目完成进度
---

Annuuser

<!-- more -->

> 加油哇



优化hive和hadoop配置

完成hive sql查询

neo4j建立数据结构

完成neo4j sql查询

前端数值展示

数值统计画图

建立分销系统界面（功能、操作）

服务器写sql

连接zookeeper

跑算法生成数据

搭服务器完成请求返回







其他优化：

mapred.reduce.slowstart.completed.maps  map有很长的一段时间是和reduce进程共存的，共存的时间取决于你设置的 如果你设置的map和reduce参数都很大，势必造成map和reduce争抢资源，造成有些进程饥饿，超时出错，最大的可能就是socket.timeout的出错，网络过于繁忙。

map和reduce之间是怎样的配比比较好呢？apache官网给了我们一些建议，比如设置reduce与map，他们之间有一个具体的公式。但是实际情况总是不能用公式来套用的（否则就不需要系统工程师了…）。一般情况下，当你设置好map和reduce进程数后，你可以通过hadoop的mapred的页面入口（http://namenode:50030/jobdetai.jps）查看map和reduce进度，如果你发现reduce在33%时，map正好提早一点点到100%，那么这将是最佳的配比，因为reduce是在33%的时候完成了copy阶段，也就是说，map需要再reduce到达33%之前完成所有的map任务，准备好数据。千万不能让reduce在等待，但是可以让map先完成。

启动分布式Job，无论多小的数据量，执行时间一般不会少于20s，使用本地mr模式，10秒左右就能出结果。

Hadoop的Map Reduce Job可以有3种模式执行， 即本地模式，伪分布式，还有真正的分布式。启动分布式Job会消耗大量资源，而真正执行计算的时间反而非常少。

​	•	Hive.exec.mode.local.auto，把他设为true就能够自动开启local mr模式

​	•	Map处理的文件数不超过4个

​	•	总大小小于128MB就启用local mr模式

JVM重用

MapReduce启动的JVM在完成一个task之后就退出了，但是如果任务花费时间很短， 又要多次启动JVM的情况下（比如对很大数据量进行计数操作）， JVM的启动时间就会变成一个比较大的overhead。

优化策略：

Hadoop的Map Reduce Job可以有3种模式执行， 即本地模式，伪分布式，还有真正的分布式。启动分布式Job会消耗大量资源，而真正执行计算的时间反而非常少。

使用jvm重用的参数： set Mapred.Job.reuse.jvm.num.tasks = 5;

io.sort.mb和mapred.child.java.opts。因为每一个map或是reduce进程都是一个task，都会对应启动一个JVM，所以其实java.opts也与你启动的map和reduce数以及别的一些jvm敏感的参数有关。既然task运行在JVM里面，那么，我这里所要提到的sort.mb 也是分配在JVM中的，这个值是用来设置到底我一个map sort的可用buffer大小是多少，如果map在内存中sort的结果达到一个特定的值，就会被spill进入硬盘。具体这个值是等于mb*io.sort.spill.percent.。按照通常的设置方式，为了让jvm发挥最佳性能，一般设置JVM的最大可用内存量为mb设置的内存量的两倍。那么mb的内存量又根据什么设置呢？它主要是与你的一个map的结果数据量有关。如果一个map的结果数据量为600M，那么如果你设置的mb*io.sort.spill.percent.=200M，那么将进行3次spill进入硬盘，然后map完成后再将数据从硬盘上取出进行copy。所以，这个mb设置如果是600M的话，那么就不需要进行这次硬盘访问了，节省了很多时间。但是最大的问题是内存耗费很大。如果mb是600M，那么jvm.opts将需要设置为1G以上，那么，按照上例，你同时启动16个map和8个reduce 的话，那么你的内存至少应该有24G。所以，这里的设置也要慎重，因为毕竟你的服务器还要跑很多其他的服务。

实验

l  Job合并输出小文件

sethive.merge.smallfiles.avgsize=256000000;

当输出文件平均大小小于该值，启动新job用于合并文件。

对于多个job时，前一个job输出很多大小不均匀的数据文件，对后续的job处理会造成数据倾斜的问题。

如果输出文件大小均匀，则后续处理的mapper数比较合理。

sethive.merge.size.per.task=64000000;

合并之后的文件大小。

优化策略：

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat

多个小文件由一个map执行。

合并文件数由mapred.max.split.size限制的大小决定。

合并小文件

　　我们知道文件数目小，容易在文件存储端造成瓶颈，给 HDFS 带来压力，影响处理效率。对此，可以通过合并Map和Reduce的结果文件来消除这样的影响。

　　用于设置合并属性的参数有：

​	•	是否合并Map输出文件：hive.merge.mapfiles=true（默认值为真）

​	•	是否合并Reduce 端输出文件：hive.merge.mapredfiles=false（默认值为假）

​	•	合并文件的大小：hive.merge.size.per.task=256*1000*1000（默认值为 256000000）

实验

MapReduce启动的JVM在完成一个task之后就退出了，但是如果任务花费时间很短， 又要多次启动JVM的情况下（比如对很大数据量进行计数操作）， JVM的启动时间就会变成一个比较大的overhead。

优化策略：

Hadoop的Map Reduce Job可以有3种模式执行， 即本地模式，伪分布式，还有真正的分布式。启动分布式Job会消耗大量资源，而真正执行计算的时间反而非常少。

使用jvm重用的参数： set Mapred.Job.reuse.jvm.num.tasks = 5;





实验

子查询。当需要执行多个子查询union all或者join操作的时候，Job间并行就可以使用了。比如下面的代码就是一个可以并行的场景示意：select * from ( select count(*) from logs where log_date = 20130801 and item_id = 1 union all select count(*) from logs where log_date = 20130802 and item_id = 2 union all select count(*) from logs where log_date = 20130803 and item_id = 3 )t。

优化策略：

设置Job间并行的参数是Hive.exec.parallel，将其设为true即可。默认的并行度为8，也就是最多允许sql中8个Job并行。如果想要更高的并行度，可以通过Hive.exec.parallel. thread.number参数进行设置，但要避免设置过大而占用过多资源。





实验

SQL优化 另外在实际开发过程中也发现，一些实现思路会导致生成多余的Job而显得不够高效。比如这个需求：查询某网站日志中访问过页面a和页面b的用户数量。低效的思路是面向明细的，先取出看过页面a的用户，再取出看过页面b的用户，然后取交集，代码如下： select count() from (select distinct user_id from logs where page_name = ‘a’) a join (select distinct user_id from logs where blog_owner = ‘b’) b on a.user_id = b.user_id; 这样一来，就要产生2个求子查询的Job，一个用于关联的Job，还有一个计数的Job，一共有4个Job。

优化策略：

但是我们直接用面向统计的方法去计算的话（也就是用*group by*替代*join*），则会更加符合*M/R*的模式，而且生成了一个完全不带子查询的*sql*，只需要用一个*Job*就能跑完：* select count(*) from logs group by user_id having (count(case when page_name = ‘a’ then 1 end) > 0 and count(case when page_name = ‘b’ then 1 end) > 0) 第一种查询方法符合思考问题的直觉，是工程师和分析师在实际查数据中最先想到的写法，但是如果在目前Hive的query planner不是那么智能的情况下，想要更加快速的跑出结果，懂一点工具的内部机理也是必须的。





HDFS：

dfs.block.size

Mapredure：

io.sort.mb

io.sort.spill.percent

mapred.local.dir

mapred.map.tasks & mapred.tasktracker.map.tasks.maximum

mapred.reduce.tasks & mapred.tasktracker.reduce.tasks.maximum

mapred.reduce.max.attempts

mapred.reduce.parallel.copies

mapreduce.reduce.shuffle.maxfetchfailures

mapred.child.java.opts

mapred.reduce.tasks.speculative.execution

mapred.compress.map.output & mapred.map.output.compression.codec



最终每个task处理的数据大致相同，均衡IO负载，以达到资源最佳使用。



**SQL****查询优化**

group by

l  Map端部分聚合：

​                   并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。

 

l  Map端部分聚合参数：

​    #是否在map端进行聚合

​                   hive.map.aggr=true;

 

l  有数据倾斜的时候进行负载均衡

\#如果group by过程中出现倾斜，应该设置为true

​    hive.groupby.skewindata=true;

​                  

\#在map端进行聚合操作的条目数目

​                   #这个是group by的键对应的记录条数超过这个值则会进行优化

​                   hive.groupby.mapaggr.checkinterval=100000;

 

和mapjoin类似，group by优化后也会启动两个Job。

当选项设为true时，生成的查询计划会有两个MR job，第一个MR job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group by key有可能被分发到不同的Reduce中，从而达到负载均衡的目的。

第二个MR job再根据预处理的数据结果按照group bykey分布到Reduce中（这个过程可以保证相同的group by 被分发到同一个Reduce中），最后完成最终的聚合操作。

 

Count(distinct)

l  当该字段存在大量值为null或空的记录时容易造成倾斜。

解决思路：

1）  count(distinct)时，将值为空的数据在where里过滤掉，在最后结果中加1。

2）  如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行[union](http://www.yidianzixun.com/m/channel/keyword/union)。

3）  如果group by维度过小，则可以  采用count和group by的方式来替换count(distinct)完成计算

l  特殊情况特殊处理：

在业务逻辑优化效果不大情况下，有些时候是可以将倾斜的数据单独拿出来处理，最后union回去。

 

l  countdistinct优化：

实例1：

​         优化前：

​         selectcount(distinct id) from student;

​         只有一个job任务，而且只有一个reduce，处理的工作量比较大。

​        

优化后：

​         selectcount(1) from (select distinct id from student) tmp;

​         或

​         selectcount(1) from (select id from student group by id) tmp;

​          可以通过设置set mapred.reduce.tasks的值，加快(select distinct id from student) tmp部分的处理。

 

实例2：

优化前：

selecta,sum(b),count(distinct c),count(distinct d)

from test

group by a;

优化后：

select a,sum(b)as b,count(c) as c,count(d) as d

from (

select a,0 as b,c,null as d from test group by a,c

union all

select a,0 as b,null as c,d from test group by a,d

union all

select a,b,null as c,null as d from test

)tmp group bya;

 

笛卡尔积

尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。

 

之前我有遇到过一种情况，不得不使用笛卡尔积，表关联的条件为不等式，还好两张表不大。如果你不得不使用笛卡尔积，那么一定要看一下其中的表是否符合Mapjoin的要求，如果符合，那么一定要使用Mapjoin。

提前裁剪数据，避免资源浪费

join优化前：

select o.cid,c.id

from order o

join customer c

on o.cid = c.id

**where o.dt = '2015-07-26';**

 

join优化后：

select o.cid,c.id

from

(select cid

from order

where dt = '2015-07-26'

)o

join customer c

on o.cid = c.id;

 

对一些过滤条件，能尽早过滤的就尽早过滤，减少IO资源浪费。

这个需要个人工作中注意就好了。



l  并行化执行

每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间。

sethive.exec.parallel=true;

sethive.exec.parallel.thread.number=15;

 

实例：

select num

from

(selectcount(city) as num

from city

union all

selectcount(province) as num

fromprovince)tmp;

 

union all两侧的查询语句会同时执行。

 

l  本地化执行（感觉生产环境用处不大）

sethive.exec.mode.local.auto=true;

当一个job满足如下条件才能真正使用本地模式：

a、   job的输入数据大小必须小于参数：

hive.exec.mode.local.auto.inputbytes.max(默认128MB)

 

b、   job的map数必须小于参数：

hive.exec.mode.local.auto.tasks.max(默认为4)

 

c、  job的reducer数必须为0或者1

 

如果你的环境不满足上述条件时，执行过程会提示原因，如Input size大于hive.exec.mode.local.auto.inputbytes.max值，或input files个数大于hive.exec.mode.local.auto.tasks.max值，同时会取消本地化执行，改为其他方式执行。

 

l  JVM重利用

set mapred.job.reuse.jvm.num.tasks=15;

JVM重利用可以是Job长时间保留slot，直到作业结束，这对于有较多任务和较多小文件的任务是非常有意义的，因为减少了JVM的启动和初始化时间，从而减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他作业可能就需要等待。



4.1列裁剪

　　Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其它列。 例如，若有以下查询：

SELECT a,b FROM q WHERE e<**10**;

　　在实施此项查询中，Q 表有 5 列（a，b，c，d，e），Hive 只读取查询逻辑中真实需要 的 3 列 a、b、e，而忽略列 c，d；这样做节省了读取开销，中间表存储开销和数据整合开销。

　　裁剪所对应的参数项为：hive.optimize.cp=true（默认值为真）

4.2分区裁剪

　　可以在查询的过程中减少不必要的分区。 例如，若有以下查询：

SELECT * FROM (SELECTT a1,COUNT(**1**) FROM T GROUP BY a1) subq WHERE subq.prtn=**100**; #（多余分区） 

SELECT * FROM T1 JOIN (SELECT * FROM T2) subq ON (T1.a1=subq.a2) WHERE subq.prtn=**100**;

　　查询语句若将“subq.prtn=100”条件放入子查询中更为高效，可以减少读入的分区 数目。 Hive 自动执行这种裁剪优化。

　　分区参数为：hive.optimize.pruner=true（默认值为真）

4.3JOIN操作

　　在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边。 因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表 可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则。 若一条语句中有多个 Join，依据 Join 的条件相同与否，有不同的处理方法。

4.3.1JOIN原则

　　在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句中有多个 Join 的情况，如果 Join 的条件相同，比如查询：.4MAP JOIN操作

　　Join 操作在 Map 阶段完成，不再需要Reduce，前提条件是需要的数据在 Map 的过程中可以访问到。比如查询： 

GROUP BY操作

　　进行GROUP BY操作时需要注意一下几点：

​	•	Map端部分聚合



4.1列裁剪

　　Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其它列。 例如，若有以下查询：

SELECT a,b FROM q WHERE e<**10**;

　　在实施此项查询中，Q 表有 5 列（a，b，c，d，e），Hive 只读取查询逻辑中真实需要 的 3 列 a、b、e，而忽略列 c，d；这样做节省了读取开销，中间表存储开销和数据整合开销。

　　裁剪所对应的参数项为：hive.optimize.cp=true（默认值为真）

4.2分区裁剪

　　可以在查询的过程中减少不必要的分区。 例如，若有以下查询：

SELECT * FROM (SELECTT a1,COUNT(**1**) FROM T GROUP BY a1) subq WHERE subq.prtn=**100**; #（多余分区） 

SELECT * FROM T1 JOIN (SELECT * FROM T2) subq ON (T1.a1=subq.a2) WHERE subq.prtn=**100**;

　　查询语句若将“subq.prtn=100”条件放入子查询中更为高效，可以减少读入的分区 数目。 Hive 自动执行这种裁剪优化。

　　分区参数为：hive.optimize.pruner=true（默认值为真）

4.3JOIN操作

　　在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边。 因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表 可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则。 若一条语句中有多个 Join，依据 Join 的条件相同与否，有不同的处理方法。

4.3.1JOIN原则

　　在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句中有多个 Join 的情况，如果 Join 的条件相同，比如查询：.4MAP JOIN操作

　　Join 操作在 Map 阶段完成，不再需要Reduce，前提条件是需要的数据在 Map 的过程中可以访问到。比如查询： 

GROUP BY操作

　　进行GROUP BY操作时需要注意一下几点：

​	•	Map端部分聚合





整体的优化策略如下：

​	•	    去除查询中不需要的column

​	•	    Where条件判断等在TableScan阶段就进行过滤

​	•	    利用Partition信息，只读取符合条件的Partition

​	•	    Map端join，以大表作驱动，小表载入所有mapper内存中

​	•	    调整Join顺序，确保以大表作为驱动表

​	•	    对于数据分布不均衡的表Group by时，为避免数据集中到少数的reducer上，分成两个map-reduce阶段。第一个阶段先用Distinct列进行shuffle，然后在reduce端部分聚合，减小数据规模，第二个map-reduce阶段再按group-by列聚合。

​	•	    在map端用hash进行部分聚合，减小reduce端数据处理规模。